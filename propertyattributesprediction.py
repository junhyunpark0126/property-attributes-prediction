# -*- coding: utf-8 -*-
"""PropertyAttributesPrediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U8g92QyRKzyN2gajyp4umYaUxjA5vZqq

# Machine Learning : Predicting the Attributes of Properties

### Imports/Setup
"""

# import packages
import json
import glob
import pandas as pd
import numpy as np
import datetime as dt
import re
import os
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import cm
from google.colab import drive
from sklearn.model_selection import train_test_split
from collections import Counter
import seaborn as sns

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !apt update
# !pip install kaggle

"""# **Part I:** Preprocessing and Modeling in `scikit-learn`

## **1.1** Data Loading and Preprocessing

### **1.1.1** Read and Load Data

We are using one CSV `properties_data.csv` from a Kaggle [dataset](https://www.kaggle.com/datasets/dataregress/dubai-properties-dataset/data). The dataset contains 38 columns and over 1900 property entries.
"""

from google.colab import drive
drive.mount('/content/drive')

!mkdir ~/.kaggle

!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/

# Download dataset
!!kaggle datasets download -d dataregress/dubai-properties-dataset

# Unzip folder in Colab content folder
!unzip /content/dubai-properties-dataset.zip

df_properties = pd.read_csv("properties_data.csv")

"""## **1.2** EDA

### **1.2.1** Visualization [12 points]

#### (a) Examining Neighborhood and Quality

For the ***top ten neighborhoods with the most properties***, we want to find the number of Low, Medium, High, Ultra listings from the `quality` column.
"""

df_temp = df_properties.groupby('neighborhood').agg({'price': 'count'}).reset_index()
df_temp = df_temp.sort_values(by='price', ascending=False)

top_ten_neighborhoods = df_temp['neighborhood'].head(10).to_list()

ten_neighborhoods_df = df_properties[df_properties['neighborhood'].isin(top_ten_neighborhoods)]

plt.figure(figsize=(8,6))
sns.countplot(data=ten_neighborhoods_df, x='neighborhood', hue='quality', palette='husl')
plt.title('Number of Property Listings By Quality In The Top 10 Neighborhoods')
plt.xlabel('Neighborhoods')
plt.ylabel('Number of Properties')
plt.legend(title='Quality')
plt.xticks(rotation=90)

plt.show()

"""#### (b) 3D Scatterplot

We want to examine the relationship between three variables: `number_of_bedrooms`, `number_of_bathrooms`, and `price`. We also want to examine `quality` as well.
"""

import matplotlib.pyplot as plt

# save the corresponding series from dataframe into lists/containers
number_of_bedrooms = df_properties['no_of_bedrooms'].to_list()
number_of_bathrooms = df_properties['no_of_bathrooms'].to_list()
price = df_properties['price'].to_list()
quality = df_properties['quality'].to_list()

# create a 3D scatter plot of size (6,6)
fig = plt.figure(figsize=(6,6))
ax = fig.add_subplot(projection='3d')

# create a dictionary with your quality and corresponding colors
color_dict = {
    'Low' : 'red',
    'Medium' : 'green',
    'High' : 'blue',
    'Ultra' : 'magenta'
}

# iterate through and plot datapoints

for i in range(len(quality)):
  ax.scatter(number_of_bedrooms[i], number_of_bathrooms[i], price[i], color=color_dict[quality[i]], marker='o')


# set title and labels

ax.set_title('Properties By Quality and Their Attributes')
ax.set_xlabel('Number of Bedrooms')
ax.set_ylabel('Number of Bathrooms')
ax.set_zlabel('Price')
ax.set_box_aspect(aspect=None, zoom=0.85)

for quality, color in color_dict.items():
    ax.scatter([], [], [], c=color, marker='o', label=quality)
ax.legend(loc='upper left')
plt.show()

"""### **1.2.2** Correlation of Feature Variables

_**Isolating Numerics from Categorical Features**_
"""

temp_df1 = df_properties.select_dtypes(include=['int64', 'float64'])
temp_df1 = temp_df1.drop(['id', 'latitude', 'longitude', 'price'], axis=1)
temp_df2 = df_properties.select_dtypes(include=['bool', 'object'])
temp_df2 = temp_df2.drop(['quality'], axis=1)

num_df = temp_df1.sort_index(axis=1, ascending=True)
cat_df = temp_df2.sort_index(axis=1, ascending=True)

num_df.columns

cat_df.columns

"""_**Correlation Heatmap**_"""

corr_mat = num_df.corr()

plt.figure(figsize=(8,8))
sns.heatmap(corr_mat, annot=True, cmap='RdBu', vmin=-1, vmax=1)
plt.title('Correlation Heatmap for Various Numeric Features')

plt.show()

"""## **1.3** Feature Engineering

### **1.3.1** Cast Boolean Values into Integers
"""

encoded_df_properties = df_properties.drop(columns=['id', 'latitude', 'longitude', 'neighborhood'])

encoded_df_properties = encoded_df_properties.apply(lambda x : x.astype(int) if x.dtype=='bool' else x)

"""### **1.3.2** Encode Classes in 'Quality' Column"""

quality_mapping = {
    'Low': 0,
    'Medium': 1,
    'High': 2,
    'Ultra': 3,
}

encoded_df_properties['quality'] = encoded_df_properties['quality'].apply(lambda x : quality_mapping[x])

encoded_df_properties['quality'].unique()

"""## **1.4** Modeling (sklearn)

### **1.4.1** Preprocessing: Create Features and Label and Split Data into Train and Test

Now that we have explored and cleaned our dataset, let's prepare it for a machine learning task.
"""

features = encoded_df_properties.drop(columns=['quality'])

target = encoded_df_properties['quality']

seed = 42
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=seed)

"""### **1.4.2** Classification Models

#### (a) Logistic Regression
"""

from sklearn.linear_model import LogisticRegression

classifier = LogisticRegression(penalty=None, multi_class = 'multinomial')
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

log_acc = classifier.score(X_test, y_test)

log_acc

"""#### (b) Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix

tree_classifier = RandomForestClassifier(class_weight = 'balanced', n_estimators = 120, max_depth = 30, random_state = 42)
tree_classifier.fit(X_train, y_train)

y_pred = tree_classifier.predict(X_test)

rf_acc = tree_classifier.score(X_test, y_test)

rf_confusion = confusion_matrix(y_test, y_pred)

rf_acc, rf_confusion

"""#### (c) PCA to Reduce Dimensionality

_**Initial PCA**_
"""

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

pca = PCA()
X_train_pca = pca.fit(X_train_scaled)

"""_**Cumulative Explained Variance Ratios**_"""

explained_variance_ratios = pca.explained_variance_ratio_

cum_evr = explained_variance_ratios.cumsum()

plt.figure(figsize=(8,3))
plt.plot(range(1, len(cum_evr) + 1), cum_evr)
plt.axhline(y=0.8, color='r')

plt.title('Cumulative Explained Variance Ratios vs. Number of Principal Components')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance Ratios')

plt.xticks(range(1, len(cum_evr) + 1, 2))

plt.show()

"""_**Final PCA**_"""

pca = PCA(n_components=16)
pca.fit(X_train_scaled)


X_test_pca = pca.transform(X_test_scaled)
X_train_pca = pca.transform(X_train_scaled)

"""#### (d) Logistic Regression with PCA"""

log_reg_pca = LogisticRegression(penalty = None, multi_class = 'multinomial')
log_reg_pca.fit(X_train_pca, y_train)

y_pred = log_reg_pca.predict(X_test_pca)

test_accuracy = log_reg_pca.score(X_test_pca, y_test)

test_accuracy

"""### **1.4.3.0** Regression: Split Data into Train and Test"""

reg_df_properties = encoded_df_properties[['price', 'size_in_sqft', 'no_of_bedrooms', 'no_of_bathrooms']]

features = reg_df_properties.drop(columns=['price'])

target = reg_df_properties['price']

seed = 42
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=seed)

"""### **1.4.3.1** Regression Models

In this section, we will switch from classification models to regression models.

Let's use the features we created in 1.4.3.0 to create regression models and predict the average price per room.

#### (a) Linear Regression (Unregularized)
"""

from sklearn.linear_model import LinearRegression

reg = LinearRegression()
reg.fit(X_train, y_train)

y_pred = reg.predict(X_test)


lin_reg_score = reg.score(X_test, y_test)

"""#### (b) Lasso Regression"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn.linear_model import Lasso

reg_lasso = Lasso(alpha = 0.5)
reg_lasso.fit(X_train_scaled, y_train)

y_pred = reg_lasso.predict(X_test_scaled)

lasso_score = reg_lasso.score(X_test_scaled, y_test)

lasso_score

"""### **1.4.4** K-Means Clustering

#### (a) Find the best number of clusters with the elbow plot
"""

features = encoded_df_properties.drop(columns=['quality'])

from sklearn.cluster import KMeans

wcss = []

for i in range(2,11):
  kmeans = KMeans(n_clusters = i, n_init = 5, random_state = 0)
  kmeans.fit(features)
  wcss.append(kmeans.inertia_)

plt.figure(figsize=(8,6))
plt.plot(range(2,11), wcss)

plt.title('Elbow Plot : WCSS vs. Number of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS Value')

plt.show()

number_of_clusters = 5

"""#### (b) Re-fit with the best number of clusters"""

kmeans = KMeans(n_clusters=5, n_init=5, random_state=0).fit(features)

"""# **Part II:** Distributed Machine Learning with Spark"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# !apt install libkrb5-dev
# !wget https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz
# !tar xf spark-3.1.2-bin-hadoop3.2.tgz
# !pip install findspark
# !pip install sparkmagic
# !pip install pyspark
# ! pip install pyspark --user
# ! pip install seaborn --user
# ! pip install plotly --user
# ! pip install imageio --user
# ! pip install folium --user

import pyspark
from pyspark.sql import SQLContext
from pyspark.sql import SparkSession
from pyspark.sql.types import *
import pyspark.sql.functions as F

spark = SparkSession.builder.appName('bigdata-hw4').getOrCreate()
sqlContext = SQLContext(spark)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext sparkmagic.magics

# Graph section
import networkx as nx

# SQLite RDBMS
import sqlite3

import os
os.environ['SPARK_HOME'] = '/content/spark-3.1.2-bin-hadoop3.2'
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

"""## **2.1** Initializing Spark Data

### **2.1.0** Converting the Pandas Dataframe into a Spark Dataframe
"""

properties_sdf = spark.createDataFrame(encoded_df_properties)

from pyspark.sql.functions import col
properties_sdf = properties_sdf.withColumn('quality', col('quality').cast(DoubleType()))

"""### **2.1.1** Setting Up a VectorAssembler"""

all_columns = properties_sdf.columns
print(all_columns)

drop_columns = properties_sdf.drop('quality', 'neighborhood')

feature_columns = drop_columns.columns
print(feature_columns)

from pyspark.ml.feature import VectorAssembler
assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')

"""## **2.2** Preprocessing: Pipeline and Train-Test Split

"""

from pyspark.ml import Pipeline

pipeline = Pipeline(stages=[assembler])

processed_properties_sdf = pipeline.fit(properties_sdf).transform(properties_sdf)

"""Now that we have the data in the format we need, we will create our train and test sets."""

random_seed = 42

train_sdf, test_sdf = processed_properties_sdf.randomSplit([0.8, 0.2], seed = random_seed)

"""## **2.3** Modelling (SparkML)

### **2.3.1** Vanilla Logistic Regression
"""

from pyspark.ml.classification import LogisticRegression

lr_model = LogisticRegression(featuresCol='features', labelCol='quality', maxIter=5, family='multinomial').fit(train_sdf)

train_accuracy = lr_model.summary.accuracy

train_accuracy

"""Now, let's find out how good the model actually is and see if it overfits to the training data."""

predictions = lr_model.transform(test_sdf)

from pyspark.mllib.evaluation import MulticlassMetrics

predictionAndLabels = predictions.select(['prediction', 'quality']).rdd

metrics = MulticlassMetrics(predictionAndLabels)

confusion_matrix = np.array(metrics.confusionMatrix().toArray())

total_correct = confusion_matrix.trace()
total = confusion_matrix.sum()
test_accuracy = total_correct / total
test_accuracy

"""### **2.3.2** Regularized Logistic Regression

Now, we will add regularization â€“ LASSO (L1), Ridge (L2) and elastic net (combination of L1 and L2), to avoid overfitting.

#### (a) LASSO (L1)
"""

l1_model = LogisticRegression(featuresCol='features', labelCol='quality', regParam = 0.1, elasticNetParam=1, maxIter=5, family='multinomial')

l1_model = l1_model.fit(train_sdf)


l1_train_accuracy = l1_model.summary.accuracy

l1_train_accuracy

predictions = l1_model.transform(test_sdf)
predictionsAndLabels = predictions.select(['prediction', 'quality']).rdd
metrics = MulticlassMetrics(predictionAndLabels)
confusion_matrix_l1 = np.array(metrics.confusionMatrix().toArray())

total_correct = confusion_matrix_l1.trace()
total = confusion_matrix_l1.sum()
l1_test_accuracy = total_correct / total

l1_test_accuracy, confusion_matrix_l1

"""#### (b) Ridge (L2)"""

l2_model = LogisticRegression(featuresCol='features', labelCol='quality', regParam = 0.1, elasticNetParam=0, maxIter=5, family='multinomial')

l2_model = l2_model.fit(train_sdf)

l2_train_accuracy = l2_model.summary.accuracy
l2_train_accuracy

predictions = l2_model.transform(test_sdf)
predictionsAndLabels = predictions.select(['prediction', 'quality']).rdd
metrics = MulticlassMetrics(predictionsAndLabels)
confusion_matrix_l2 = np.array(metrics.confusionMatrix().toArray())

total = confusion_matrix_l2.sum()
total_correct = confusion_matrix_l2.trace()
l2_test_accuracy = total_correct / total
l2_test_accuracy, confusion_matrix_l2

"""#### (c) Elastic Net"""

en_model = LogisticRegression(featuresCol='features', labelCol='quality', regParam = 0.1, elasticNetParam=0.15, maxIter=5, family='multinomial')

en_model = en_model.fit(train_sdf)

en_train_accuracy = en_model.summary.accuracy

en_train_accuracy

predictions = en_model.transform(test_sdf)
predictionsAndLabels = predictions.select(['prediction', 'quality']).rdd
metrics = MulticlassMetrics(predictionsAndLabels)
confusion_matrix_en = np.array(metrics.confusionMatrix().toArray())

total = confusion_matrix_en.sum()
total_correct = confusion_matrix_en.trace()
en_test_accuracy = total_correct / total
en_test_accuracy, confusion_matrix_en

"""### **2.3.3** Random Forest Classification"""

from pyspark.ml.classification import RandomForestClassifier


random_seed = 42
rf = RandomForestClassifier(seed=random_seed, maxDepth=10, labelCol='quality')
rf_model = rf.fit(train_sdf)

train_pred = rf_model.transform(train_sdf)
test_pred = rf_model.transform(test_sdf)

predictionsAndLabels = train_pred.select(['prediction', 'quality']).rdd
metrics = MulticlassMetrics(predictionsAndLabels)
rf_train_cm = np.array(metrics.confusionMatrix().toArray())
total_correct = rf_train_cm.trace()
total = rf_train_cm.sum()
rf_train_accuracy = total_correct / total

predictionsAndLabels = test_pred.select(['prediction', 'quality']).rdd
metrics = MulticlassMetrics(predictionsAndLabels)
rf_test_cm = np.array(metrics.confusionMatrix().toArray())
total_correct = rf_test_cm.trace()
total = rf_test_cm.sum()
rf_test_accuracy = total_correct / total
rf_test_accuracy

"""### **2.3.4** Dimensionality Reduction Using PCA"""

from pyspark.ml.feature import PCA, StandardScaler

scaler = StandardScaler(inputCol='features', outputCol='scaled_features')
scaler_model = scaler.fit(train_sdf)
train_sdf_scaled = scaler_model.transform(train_sdf)
test_sdf_scaled = scaler_model.transform(test_sdf)

pca = PCA(k=16, inputCol='scaled_features', outputCol='pca_features')
pca_model = pca.fit(train_sdf_scaled)

train_sdf_pca = pca_model.transform(train_sdf_scaled)
test_sdf_pca = pca_model.transform(test_sdf_scaled)

"""Now, we create a Logistic Regression model and train it using the PCA features."""

lr_model = LogisticRegression(featuresCol='features', labelCol='quality', maxIter=5, family='multinomial').fit(train_sdf_pca)

test_pred = lr_model.transform(test_sdf_pca)

train_accuracy_pca = lr_model.summary.accuracy

predictionsAndLabels = test_pred.select(['prediction', 'quality']).rdd
metrics = MulticlassMetrics(predictionsAndLabels)
confusion_matrix_pca = np.array(metrics.confusionMatrix().toArray())

total = confusion_matrix_pca.sum()
total_correct = confusion_matrix_pca.trace()
test_accuracy_pca = total_correct / total
test_accuracy_pca